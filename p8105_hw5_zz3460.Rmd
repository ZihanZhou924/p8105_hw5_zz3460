---
title: "p8105_hw5_zz3460"
author: "Zihan Zhou"
date: "2025-11-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

## Problem 1 

Load necessary packages and set seed.

```{r}
set.seed(2025)
library(tidyverse)
library(ggplot2)
```

Define a function to solve the problem.

```{r}
has_shared_birthday <- function(n){
  bdays <- sample(1:365, size = n, replace = TRUE)
  any(duplicated(bdays))
}

reps <- 10000
ns <- 2:50

results <- tibble(
  n = ns,
  prob = map_dbl(ns, ~ mean(replicate(reps, has_shared_birthday(.x))))
)

ggplot(results, aes(x = n, y = prob)) +
  geom_line() +
  geom_point() +
  labs(
    title = "probability that at least two people in the group will share a birthday",
    x = "n",
    y = "prob"
  ) +
  theme_minimal(base_size = 14)
```

The plot shows that:
1. When the group size is over 23, the probability is above 50%.
2. When the group size reaches 50, the probability is close to 100%.

## Problem 2

Load key packages.

```{r}
library(broom)
library(purrr)
```

Do simulations.

```{r}
n <- 30
sigma <- 5
mus <- 0:6
reps <- 5000

simulate_for_mu <- function(mu){
  replicate(reps, {
    x <- rnorm(n, mean = mu, sd = sigma)
    t_out <- t.test(x, mu = 0)
    tidy(t_out)[c("estimate", "p.value")]
  }, simplify = FALSE) %>%
  bind_rows(.id = "sim") %>%
  mutate(mu = mu)
}

sim_all <- map_dfr(mus, simulate_for_mu)

power_by_mu <- sim_all %>%
  mutate(reject = p.value < 0.05) %>%
  group_by(mu) %>%
  summarise(power = mean(reject),
  mean_est = mean(estimate),
  mean_est_reject = mean(estimate[reject], na.rm = TRUE),
  n_reject = sum(reject))

power_by_mu
```

Make plots.

```{r}
p1 <- ggplot(power_by_mu, aes(x = mu, y = power)) +
  geom_line() + 
  geom_point() +
  labs(title = "one sample t-test (n=30, sigma=5, reps=5000)",
x = "mu", y = "power") +
  theme_minimal(base_size = 14)

p1
```

p1 shows a strong positive association between the effect size (the true value of $\mu$) and power.
- When the true $\mu$ is 0 (meaning the null hypothesis is true), the power is approximately 5%. This is the Type I error rate ($\alpha$), which is exactly what we set our significance level to. We correctly rejected the (true) null hypothesis about 5% of the time.
- As the true $\mu$ increases, moving further away from the null value of 0, the power (the probability of correctly rejecting the false null) increases rapidly.
- By the time the true $\mu$ reaches 4, the test has nearly 100% power, meaning it almost always detects the effect.

```{r}
p2 <- ggplot(power_by_mu, aes(x = mu)) +
  geom_line(aes(y = mean_est), linetype = "dashed") +
  geom_point(aes(y = mean_est)) +
  geom_line(aes(y = mean_est_reject), color = "red") +
  geom_point(aes(y = mean_est_reject), color = "red") +
  labs(title = "average estimate", x = "mu", y = "mu_hat") +
  theme_minimal(base_size = 14)

p2
```

Is the sample average of $\hat{\mu}$ across tests for which the null is rejected approximately equal to the true value of $\mu$? Why or why not?

No, the sample average of $\hat{\mu}$ among only the tests where the null is rejected is not approximately equal to the true value of $\mu$, especially when the true $\mu$ is small and power is low. This bias arises from selection based on significance. The overall average of $\hat{\mu}$ across all simulations (red line) lies exactly on the $y=x$ line, confirming that $\hat{\mu}$ is an unbiased estimator of $\mu$ in general. However, the average conditional on rejection (blue line) is biased upward. When $\mu$ is small (e.g., $\mu=1$), the only way to achieve a significant result is when random sampling produces an unusually large $\hat{\mu}$, inflating the conditional mean. As $\mu$ grows and power approaches 100%, nearly all simulations lead to rejection; thus, the “rejected only” group becomes equivalent to the full set of simulations, causing the red and blue lines to converge for large true effect sizes.

## Problem 3

1.Load key packages.

```{r}
library(tidyverse)
library(broom)
library(purrr)
```

2.Describe the data.

```{r}
homicide <- read_csv("data/homicide-data.csv")

glimpse(homicide)
summary(homicide)

```

The raw dataset contains individual homicide records from 50 major U.S. cities. Each row represents a single homicide case, including information such as the victim’s demographics, incident date, location, and case disposition. The disposition variable is particularly important because it indicates whether a homicide was solved, unsolved, or closed without an arrest. Since the dataset stores city and state in separate fields, we create a combined city_state variable to facilitate grouping and summary analyses at the city level.

3.Create city_state and summarize homicides per city.

```{r}
homicide_city <- homicide %>%
  mutate(city_state = paste(city, state, sep = ", "),
  unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")) %>%
  group_by(city_state) %>%
  summarise(
    total = n(),
    unsolved = sum(unsolved)
  ) %>%
  arrange(desc(unsolved))

homicide_city

```

After creating the city_state variable, we summarize each city by computing the total number of homicides and the number of unsolved homicides. Here, unsolved cases are defined as those with dispositions “Closed without arrest” or “Open/No arrest.” This provides a clear overview of the burden of unsolved homicides across U.S. cities and forms the basis for estimating city-specific unsolved proportions.

4.Baltimore, MD – run prop.test() and extract proportion + CI

```{r}
baltimore <- homicide_city %>%
  filter(city_state == "Baltimore, MD")

baltimore_test <- prop.test(
  x = baltimore$unsolved,
  n = baltimore$total
)

baltimore_tidy <- tidy(baltimore_test)

baltimore_tidy %>%
  select(estimate, conf.low, conf.high)

```

For Baltimore, MD, we use prop.test() to estimate the proportion of homicides that remain unsolved. The test output, once tidied using broom::tidy(), provides the estimated proportion along with its 95% confidence interval. This gives a statistically grounded estimate of Baltimore’s unsolved rate, taking into account sampling variability.

5.Run prop.test() for all cities.

```{r}
city_results <- homicide_city %>%
  mutate(
    test = map2(unsolved, total, ~ prop.test(.x, .y)),
    tidy = map(test, tidy)
  ) %>%
  unnest(tidy) %>%
  select(city_state, total, unsolved, estimate, conf.low, conf.high)

city_results

```

We repeat the proportion test for every city in the dataset using a tidy workflow that relies on purrr::map and list columns. For each city, we extract the estimated proportion of unsolved homicides and its confidence interval. The resulting tidy dataframe allows us to compare cities systematically and supports clear visualization with confidence intervals.

6.Plot estimates + CIs for each city.

```{r}
city_results_plot <- city_results %>%
  arrange(estimate) %>%
  mutate(city_state = factor(city_state, levels = city_state))

p3 <- ggplot(city_results_plot, aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(
    title = "Estimated proportion of unsolved homicides by city",
    x = "City",
    y = "Estimated proportion unsolved"
  ) +
  theme_minimal(base_size = 14)

p3
```

The plot displays each city’s estimated proportion of unsolved homicides, along with its corresponding confidence interval. Cities are ordered from lowest to highest unsolved proportion, allowing easy comparison across locations. Wide confidence intervals typically correspond to cities with fewer total homicides, whereas narrower intervals indicate more data and therefore more precise estimates. This visualization highlights substantial variation across cities in the proportion of unsolved homicide cases.